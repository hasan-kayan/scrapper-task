# scrapper-task# ğŸ•ï¸ Campground Scraper

A robust and scalable Python-based scraping service that extracts **campground data** from The Dyrt's API using bounding box queries. The system validates and stores structured data in a PostgreSQL database, and is deployable with Docker.

---

## ğŸ“¦ Features

- ğŸ” **Asynchronous Scraping** using `httpx` and bounding box parameters.
- ğŸ—ºï¸ Parses campground details: name, coordinates, accommodation types, etc.
- ğŸ§° **Validation with Pydantic** and persistence via **SQLAlchemy ORM**.
- ğŸ” **Retries with Tenacity** for robustness against API timeouts or failures.
- âš™ï¸ **FastAPI endpoint** to trigger scraping manually.
- ğŸ³ **Docker & docker-compose** support for containerized deployments.
- â±ï¸ Ready for **task scheduling** (e.g. with cron or APScheduler).

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourname/campground-scraper.git
cd campground-scraper
```

### 2. Set up environment variables

Create a `.env` file or configure your API URL and DB connection in `app/config.py`.

```env
API_BASE_URL=https://thedyrt.com/api/v6/locations/search-results
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/campgrounds
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run locally

```bash
uvicorn api.main:app --reload
```

Then visit [http://localhost:8000/docs](http://localhost:8000/docs) to access the interactive Swagger UI.

---

## ğŸ³ Docker Usage

### Build and run the container

```bash
docker-compose up --build
```

This will:
- Start the FastAPI app
- Connect to your PostgreSQL container (if defined in `docker-compose.yml`)
- Be ready to receive `/scrape` POST requests

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ api/                    # FastAPI entrypoint and routes
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py           # Settings and environment variables
â”‚   â”œâ”€â”€ db.py               # Database session handling
â”‚   â”œâ”€â”€ logger.py           # Logging configuration
â”‚   â””â”€â”€ scheduler.py        # (Optional) Scheduled task triggers
â”œâ”€â”€ models/                 # Pydantic + SQLAlchemy models
â”œâ”€â”€ scraper/                # Main scraping logic using HTTPX + bounding box
â”œâ”€â”€ services/               # Upsert and persistence logic
â”œâ”€â”€ tests/                  # Test cases for scraper and DB logic
â”œâ”€â”€ Dockerfile              # Docker config
â”œâ”€â”€ docker-compose.yml      # Compose for app + db
â””â”€â”€ requirements.txt        # Python dependencies
```

---

## ğŸ§ª Running Tests

```bash
pytest
```

Ensure a test database is set up and accessible before running tests.

---

## ğŸ“¬ API Usage

### `POST /scrape`

Trigger the scraper for the entire USA.

**Request:**
```bash
curl -X POST http://localhost:8000/scrape
```

**Response:**
```json
{ "status": "started" }
```

---

## ğŸ”’ License

MIT License. Feel free to use, extend, and contribute.

---

## ğŸ™‹â€â™‚ï¸ Author

Developed by [Hasan Kayan](https://github.com/hasankayan) â€“ Contributions welcome!
